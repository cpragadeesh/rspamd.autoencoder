{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = json.loads(open(\"../data/processed/dataset.txt\", 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails: 3048\n",
      "Number of spam: 1397\n",
      "Number of ham: 1651\n",
      "Number of unique symbols: 204\n"
     ]
    }
   ],
   "source": [
    "print \"Number of emails: \" + str(y.shape[0])\n",
    "print \"Number of spam: \" + str(y[y==1].shape[0])\n",
    "print \"Number of ham: \" + str(y[y==-1].shape[0])\n",
    "\n",
    "print \"Number of unique symbols: \" + str(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2286\n",
      "Testing samples: 762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print \"Training samples: \" + str(X_train.shape[0])\n",
    "print \"Testing samples: \" + str(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56270183\n",
      "Iteration 2, loss = 0.34694637\n",
      "Iteration 3, loss = 0.22797752\n",
      "Iteration 4, loss = 0.16147935\n",
      "Iteration 5, loss = 0.12100960\n",
      "Iteration 6, loss = 0.09440342\n",
      "Iteration 7, loss = 0.07567211\n",
      "Iteration 8, loss = 0.06131743\n",
      "Iteration 9, loss = 0.05064884\n",
      "Iteration 10, loss = 0.04217843\n",
      "Iteration 11, loss = 0.03514878\n",
      "Iteration 12, loss = 0.02960245\n",
      "Iteration 13, loss = 0.02512204\n",
      "Iteration 14, loss = 0.02156454\n",
      "Iteration 15, loss = 0.01860246\n",
      "Iteration 16, loss = 0.01606648\n",
      "Iteration 17, loss = 0.01399743\n",
      "Iteration 18, loss = 0.01229197\n",
      "Iteration 19, loss = 0.01090191\n",
      "Iteration 20, loss = 0.00964659\n",
      "Iteration 21, loss = 0.00861611\n",
      "Iteration 22, loss = 0.00771403\n",
      "Iteration 23, loss = 0.00694796\n",
      "Iteration 24, loss = 0.00629417\n",
      "Iteration 25, loss = 0.00567841\n",
      "Iteration 26, loss = 0.00519335\n",
      "Iteration 27, loss = 0.00475442\n",
      "Iteration 28, loss = 0.00435414\n",
      "Iteration 29, loss = 0.00402405\n",
      "Iteration 30, loss = 0.00371889\n",
      "Iteration 31, loss = 0.00344280\n",
      "Iteration 32, loss = 0.00319722\n",
      "Iteration 33, loss = 0.00298354\n",
      "Iteration 34, loss = 0.00277743\n",
      "Iteration 35, loss = 0.00260326\n",
      "Iteration 36, loss = 0.00244532\n",
      "Iteration 37, loss = 0.00229537\n",
      "Iteration 38, loss = 0.00215880\n",
      "Iteration 39, loss = 0.00205244\n",
      "Iteration 40, loss = 0.00193082\n",
      "Iteration 41, loss = 0.00182636\n",
      "Iteration 42, loss = 0.00173098\n",
      "Iteration 43, loss = 0.00164351\n",
      "Iteration 44, loss = 0.00156480\n",
      "Iteration 45, loss = 0.00148847\n",
      "Iteration 46, loss = 0.00142237\n",
      "Iteration 47, loss = 0.00135613\n",
      "Iteration 48, loss = 0.00129531\n",
      "Iteration 49, loss = 0.00123931\n",
      "Iteration 50, loss = 0.00118630\n",
      "Iteration 51, loss = 0.00113831\n",
      "Iteration 52, loss = 0.00109136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,), activation='relu', solver='adam', batch_size=500, verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham correctly classified: 411\n",
      "Number of spam correctly classified: 349\n",
      "Number of ham incorrectly classified: 2\n",
      "Number of spam incorrectly classified: 0\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print \"Number of ham correctly classified: \" + str(tn)\n",
    "print \"Number of spam correctly classified: \" + str(tp)\n",
    "print \"Number of ham incorrectly classified: \" + str(fp)\n",
    "print \"Number of spam incorrectly classified: \" + str(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
