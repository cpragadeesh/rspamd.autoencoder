{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = json.loads(open(\"../data/processed/dataset.txt\", 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails: 3048\n",
      "Number of spam: 1397\n",
      "Number of ham: 1651\n",
      "Number of unique symbols: 526\n"
     ]
    }
   ],
   "source": [
    "print \"Number of emails: \" + str(y.shape[0])\n",
    "print \"Number of spam: \" + str(y[y==1].shape[0])\n",
    "print \"Number of ham: \" + str(y[y==-1].shape[0])\n",
    "\n",
    "print \"Number of unique symbols: \" + str(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2286\n",
      "Testing samples: 762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print \"Training samples: \" + str(X_train.shape[0])\n",
    "print \"Testing samples: \" + str(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37712267\n",
      "Iteration 2, loss = 0.17747624\n",
      "Iteration 3, loss = 0.13422646\n",
      "Iteration 4, loss = 0.11804128\n",
      "Iteration 5, loss = 0.10495433\n",
      "Iteration 6, loss = 0.09363893\n",
      "Iteration 7, loss = 0.08895384\n",
      "Iteration 8, loss = 0.08058484\n",
      "Iteration 9, loss = 0.07314565\n",
      "Iteration 10, loss = 0.06887500\n",
      "Iteration 11, loss = 0.06403496\n",
      "Iteration 12, loss = 0.05886320\n",
      "Iteration 13, loss = 0.05562595\n",
      "Iteration 14, loss = 0.05404621\n",
      "Iteration 15, loss = 0.04851559\n",
      "Iteration 16, loss = 0.04559954\n",
      "Iteration 17, loss = 0.04369478\n",
      "Iteration 18, loss = 0.04140317\n",
      "Iteration 19, loss = 0.03726862\n",
      "Iteration 20, loss = 0.03597393\n",
      "Iteration 21, loss = 0.03349803\n",
      "Iteration 22, loss = 0.03108009\n",
      "Iteration 23, loss = 0.02997789\n",
      "Iteration 24, loss = 0.02799606\n",
      "Iteration 25, loss = 0.02727137\n",
      "Iteration 26, loss = 0.02548459\n",
      "Iteration 27, loss = 0.02548111\n",
      "Iteration 28, loss = 0.02404882\n",
      "Iteration 29, loss = 0.02283011\n",
      "Iteration 30, loss = 0.02230326\n",
      "Iteration 31, loss = 0.02067667\n",
      "Iteration 32, loss = 0.02125572\n",
      "Iteration 33, loss = 0.02000329\n",
      "Iteration 34, loss = 0.01805838\n",
      "Iteration 35, loss = 0.01840314\n",
      "Iteration 36, loss = 0.01748564\n",
      "Iteration 37, loss = 0.01668973\n",
      "Iteration 38, loss = 0.01665202\n",
      "Iteration 39, loss = 0.01616708\n",
      "Iteration 40, loss = 0.01559568\n",
      "Iteration 41, loss = 0.01546501\n",
      "Iteration 42, loss = 0.01456760\n",
      "Iteration 43, loss = 0.01412628\n",
      "Iteration 44, loss = 0.01516742\n",
      "Iteration 45, loss = 0.01364081\n",
      "Iteration 46, loss = 0.01354670\n",
      "Iteration 47, loss = 0.01274093\n",
      "Iteration 48, loss = 0.01345360\n",
      "Iteration 49, loss = 0.01374831\n",
      "Iteration 50, loss = 0.01345989\n",
      "Iteration 51, loss = 0.01170683\n",
      "Iteration 52, loss = 0.01138362\n",
      "Iteration 53, loss = 0.01193861\n",
      "Iteration 54, loss = 0.01096610\n",
      "Iteration 55, loss = 0.01097960\n",
      "Iteration 56, loss = 0.01195822\n",
      "Iteration 57, loss = 0.01485385\n",
      "Iteration 58, loss = 0.01258459\n",
      "Iteration 59, loss = 0.01214198\n",
      "Iteration 60, loss = 0.01133402\n",
      "Iteration 61, loss = 0.01055753\n",
      "Iteration 62, loss = 0.00949574\n",
      "Iteration 63, loss = 0.01061503\n",
      "Iteration 64, loss = 0.01025458\n",
      "Iteration 65, loss = 0.00936592\n",
      "Iteration 66, loss = 0.01201895\n",
      "Iteration 67, loss = 0.01090021\n",
      "Iteration 68, loss = 0.01010509\n",
      "Iteration 69, loss = 0.00927248\n",
      "Iteration 70, loss = 0.00993236\n",
      "Iteration 71, loss = 0.00937250\n",
      "Iteration 72, loss = 0.00972058\n",
      "Iteration 73, loss = 0.00888179\n",
      "Iteration 74, loss = 0.00926341\n",
      "Iteration 75, loss = 0.00944697\n",
      "Iteration 76, loss = 0.01023403\n",
      "Iteration 77, loss = 0.01017100\n",
      "Iteration 78, loss = 0.00848673\n",
      "Iteration 79, loss = 0.00807941\n",
      "Iteration 80, loss = 0.00870413\n",
      "Iteration 81, loss = 0.00807366\n",
      "Iteration 82, loss = 0.00884568\n",
      "Iteration 83, loss = 0.00983048\n",
      "Iteration 84, loss = 0.00875041\n",
      "Iteration 85, loss = 0.00793485\n",
      "Iteration 86, loss = 0.00812260\n",
      "Iteration 87, loss = 0.00809863\n",
      "Iteration 88, loss = 0.00825790\n",
      "Iteration 89, loss = 0.00836431\n",
      "Iteration 90, loss = 0.00827144\n",
      "Iteration 91, loss = 0.00862473\n",
      "Iteration 92, loss = 0.00764116\n",
      "Iteration 93, loss = 0.00812574\n",
      "Iteration 94, loss = 0.00803432\n",
      "Iteration 95, loss = 0.00865225\n",
      "Iteration 96, loss = 0.00787842\n",
      "Iteration 97, loss = 0.00870834\n",
      "Iteration 98, loss = 0.00765004\n",
      "Iteration 99, loss = 0.01026460\n",
      "Iteration 100, loss = 0.00957423\n",
      "Iteration 101, loss = 0.00817809\n",
      "Iteration 102, loss = 0.00780187\n",
      "Iteration 103, loss = 0.00794922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,), activation='relu', solver='adam', batch_size=100, verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham correctly classified: 402\n",
      "Number of spam correctly classified: 333\n",
      "Number of ham incorrectly classified: 11\n",
      "Number of spam incorrectly classified: 16\n",
      "Precision: 0.9680232558139535\n",
      "Recall: 0.9541547277936963\n",
      "F1-score: 0.9610389610389611\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print \"Number of ham correctly classified: \" + str(tn)\n",
    "print \"Number of spam correctly classified: \" + str(tp)\n",
    "print \"Number of ham incorrectly classified: \" + str(fp)\n",
    "print \"Number of spam incorrectly classified: \" + str(fn)\n",
    "\n",
    "precision = tp / float(tp + fp)\n",
    "recall = tp / float(tp + fn)\n",
    "\n",
    "f1 = 2 * (precision * recall) / float(precision + recall)\n",
    "\n",
    "print (\"Precision: \" + str(precision))\n",
    "print (\"Recall: \" + str(recall))\n",
    "print (\"F1-score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
